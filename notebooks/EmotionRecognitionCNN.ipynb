{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5ac2fe",
   "metadata": {},
   "source": [
    "\n",
    "# EmotionRecognitionCNN.ipynb\n",
    "\n",
    "This notebook:\n",
    "1. Load `data/cls_pool/`, create a **seeded stratified** split into `data/cls_data/` (one time).\n",
    "2. Define **BASELINE CONFIG** and train/evaluate (frozen).\n",
    "3. Define a reusable **run_experiment(config, tag)** that:\n",
    "  - builds model\n",
    "  - trains with callbacks\n",
    "  - evaluates (val/test, F1, confusion)\n",
    "  - logs to `artifacts/trials/<run_id>/` and updates `artifacts/outputs/leaderboard.csv`\n",
    "4. Define optimization framework for experimentation: **optimizations only** (override config keys), compare in leaderboard/plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f222bf",
   "metadata": {},
   "source": [
    "## Setup Imports, fixed seed, paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22642807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, platform, json, math, time, uuid, csv, shutil, random, sklearn\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm as tq\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm as pb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import itertools\n",
    "import io\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"OS:\", platform.platform())\n",
    "print(\"NumPy:\", np.__version__)\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"tqdm:\", tq.__version__)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "# Paths\n",
    "DATA = Path(\"../data\")\n",
    "POOL = DATA / \"cls_pool\"       # prepeared dataset pool\n",
    "CLS  = DATA / \"cls_data\"       # split written here\n",
    "ART  = Path(\"../artifacts\")\n",
    "MODELS = ART / \"models\"\n",
    "TRIALS = ART / \"trials\"\n",
    "OUTS   = ART / \"outputs\"\n",
    "for p in [CLS, MODELS, TRIALS, OUTS]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Class names\n",
    "with open(OUTS / \"class_names.json\", \"r\") as f:\n",
    "    CLASS_NAMES = json.load(f)\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(\"Classes:\", CLASS_NAMES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fa842",
   "metadata": {},
   "source": [
    "## Create a seeded, stratified split (70/15/15) **once**, save manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect pool paths & labels\n",
    "X, y = [], []\n",
    "for i, cname in enumerate(sorted(CLASS_NAMES)):\n",
    "    for p in (POOL / cname).glob(\"*\"):\n",
    "        if p.suffix.lower() in {\".jpg\",\".jpeg\",\".png\"}:\n",
    "            X.append(p); y.append(cname)\n",
    "X = np.array(X); y = np.array(y)\n",
    "\n",
    "# If a previous split manifest exists, reuse it (keeps trials comparable)\n",
    "split_manifest_path = OUTS / \"split_manifest.json\"\n",
    "if split_manifest_path.exists():\n",
    "    with open(split_manifest_path, \"r\") as f:\n",
    "        split_manifest = json.load(f)\n",
    "    print(\"Loaded existing split manifest.\")\n",
    "else:\n",
    "    # 70/30, then split the 30 into 15/15\n",
    "    sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.30, random_state=SEED)\n",
    "    (train_idx, temp_idx) = next(sss1.split(X, y))\n",
    "    X_train, y_train = X[train_idx], y[train_idx]\n",
    "    X_temp,  y_temp  = X[temp_idx],  y[temp_idx]\n",
    "\n",
    "    sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.50, random_state=SEED)\n",
    "    (val_idx, test_idx) = next(sss2.split(X_temp, y_temp))\n",
    "    X_val, y_val   = X_temp[val_idx],  y_temp[val_idx]\n",
    "    X_test, y_test = X_temp[test_idx], y_temp[test_idx]\n",
    "\n",
    "    # Write files into cls_data/<split>/<class> (idempotent)\n",
    "    def place(paths, labels, split):\n",
    "        print(f\"Copying {len(paths)} files to {split}/ ...\")\n",
    "        for p, lbl in pb(zip(paths, labels), total=len(paths), desc=f\"{split:>5s} split\"):\n",
    "            dst = CLS / split / lbl\n",
    "            dst.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(p, dst / p.name)\n",
    "    place(X_train, y_train, \"train\")\n",
    "    place(X_val,   y_val,   \"val\")\n",
    "    place(X_test,  y_test,  \"test\")\n",
    "\n",
    "    split_manifest = {\n",
    "        \"seed\": SEED,\n",
    "        \"splits\": {\n",
    "            \"train\": [str((CLS / \"train\" / y_train[i] / X_train[i].name).resolve()) for i in range(len(X_train))],\n",
    "            \"val\":   [str((CLS / \"val\"   / y_val[i]   / X_val[i].name).resolve())   for i in range(len(X_val))],\n",
    "            \"test\":  [str((CLS / \"test\"  / y_test[i]  / X_test[i].name).resolve())  for i in range(len(X_test))]\n",
    "        }\n",
    "    }\n",
    "    with open(split_manifest_path, \"w\") as f:\n",
    "        json.dump(split_manifest, f, indent=2)\n",
    "    print(\"Created split and saved manifest:\", split_manifest_path)\n",
    "\n",
    "# Keras datasets\n",
    "IMG = 96; BATCH = 64\n",
    "def make_ds(split):\n",
    "    return tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        CLS / split,\n",
    "        labels=\"inferred\", label_mode=\"int\",\n",
    "        color_mode=\"grayscale\", image_size=(IMG, IMG),\n",
    "        batch_size=BATCH, shuffle=True, seed=SEED)\n",
    "train_ds = make_ds(\"train\"); val_ds = make_ds(\"val\"); test_ds = make_ds(\"test\")\n",
    "class_names = train_ds.class_names\n",
    "assert class_names == CLASS_NAMES, (class_names, CLASS_NAMES)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds   = val_ds.cache().prefetch(AUTOTUNE)\n",
    "test_ds  = test_ds.cache().prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc57ae0",
   "metadata": {},
   "source": [
    "## Define **Baseline configuration**\n",
    "\n",
    "This cell will remain unchanged. All future optimizations will override keys in later defined cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920253bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE = {\n",
    "    \"img_size\": IMG,\n",
    "    \"batch_size\": BATCH,\n",
    "    \"epochs\": 60,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"l2\": 1e-4,\n",
    "    \"dropout\": 0.4,\n",
    "    \"augment\": {\"flip\": True, \"rotation\": 0.08, \"zoom\": 0.10},\n",
    "    \"filters\": [32, 64, 128, 256],\n",
    "    \"use_residual\": True,\n",
    "    \"seed\": SEED,\n",
    "}\n",
    "print(json.dumps(BASELINE, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec02cd",
   "metadata": {},
   "source": [
    "## Define model builder utility and **run_experiment(config, tag)**\n",
    "\n",
    "This utility will provide the following functionality:\n",
    "- builds the CNN from `config`\n",
    "- trains with EarlyStopping + ReduceLROnPlateau\n",
    "- evaluates on **val** and **test**\n",
    "- logs everything to `artifacts/trials/<run_id>/` and updates `artifacts/outputs/leaderboard.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc57ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, f, k=3, pool=True, l2=1e-4):\n",
    "    x = layers.Conv2D(f, k, padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    if pool: x = layers.MaxPool2D()(x)\n",
    "    return x\n",
    "\n",
    "def residual_stack(x, f, l2=1e-4):\n",
    "    sc = layers.Conv2D(f, 1, padding=\"same\", use_bias=False,\n",
    "                       kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    sc = layers.BatchNormalization()(sc)\n",
    "    y = layers.Conv2D(f, 3, padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=regularizers.l2(l2))(x)\n",
    "    y = layers.BatchNormalization()(y); y = layers.ReLU()(y)\n",
    "    y = layers.Conv2D(f, 3, padding=\"same\", use_bias=False,\n",
    "                      kernel_regularizer=regularizers.l2(l2))(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.Add()([y, sc]); y = layers.ReLU()(y)\n",
    "    y = layers.MaxPool2D()(y)\n",
    "    return y\n",
    "\n",
    "def build_model(cfg):\n",
    "    tf.random.set_seed(cfg[\"seed\"])\n",
    "    augment_layers = []\n",
    "    if cfg[\"augment\"].get(\"flip\"):     augment_layers.append(layers.RandomFlip(\"horizontal\"))\n",
    "    if cfg[\"augment\"].get(\"rotation\"): augment_layers.append(layers.RandomRotation(cfg[\"augment\"][\"rotation\"]))\n",
    "    if cfg[\"augment\"].get(\"zoom\"):     augment_layers.append(layers.RandomZoom(cfg[\"augment\"][\"zoom\"]))\n",
    "    augment = tf.keras.Sequential(augment_layers)\n",
    "\n",
    "    inp = layers.Input((cfg[\"img_size\"], cfg[\"img_size\"], 1))\n",
    "    x = augment(inp)\n",
    "    x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "    f1, f2, f3, f4 = cfg[\"filters\"]\n",
    "    x = conv_block(x, f1, l2=cfg[\"l2\"])\n",
    "    x = conv_block(x, f2, l2=cfg[\"l2\"])\n",
    "    if cfg[\"use_residual\"]:\n",
    "        x = residual_stack(x, f3, l2=cfg[\"l2\"])\n",
    "    else:\n",
    "        x = conv_block(x, f3, l2=cfg[\"l2\"])\n",
    "        x = layers.MaxPool2D()(x)\n",
    "\n",
    "    x = conv_block(x, f4, pool=False, l2=cfg[\"l2\"])\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(cfg[\"dropout\"])(x)\n",
    "    out = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
    "    model = models.Model(inp, out)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(cfg[\"learning_rate\"]) if cfg[\"optimizer\"] == \"adam\" else tf.keras.optimizers.SGD(cfg[\"learning_rate\"], momentum=0.9, nesterov=True)\n",
    "    model.compile(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def compute_class_weights(ds):\n",
    "    # count labels from train_ds\n",
    "    counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
    "    for _, yb in tf.data.Dataset.unbatch(ds.take(999999)):\n",
    "        counts[int(yb.numpy())] += 1\n",
    "    total = counts.sum()\n",
    "    return {i: float(total/(NUM_CLASSES*counts[i])) for i in range(NUM_CLASSES)}\n",
    "\n",
    "LEADERBOARD = OUTS / \"leaderboard.csv\"\n",
    "if not LEADERBOARD.exists():\n",
    "    with open(LEADERBOARD, \"w\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\"run_id\",\"tag\",\"val_acc\",\"val_f1_macro\",\"test_acc\",\"test_f1_macro\",\"epochs\",\"params_json\"])\n",
    "\n",
    "def plot_accuracy(history, out_png_path, show=True):\n",
    "    h = history.history\n",
    "    epochs = range(1, len(h[\"loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    if \"accuracy\" in h:      plt.plot(epochs, h[\"accuracy\"], label=\"train\")\n",
    "    if \"val_accuracy\" in h:  plt.plot(epochs, h[\"val_accuracy\"], label=\"val\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_png_path, dpi=160)\n",
    "    if show: plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_loss(history, out_png_path, show=True):\n",
    "    h = history.history\n",
    "    epochs = range(1, len(h[\"loss\"]) + 1)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(epochs, h[\"loss\"], label=\"train\")\n",
    "    if \"val_loss\" in h: plt.plot(epochs, h[\"val_loss\"], label=\"val\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(out_png_path, dpi=160)\n",
    "    if show: plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion(cm, class_names, out_png_path, normalize=True, cmap=\"Blues\", show=True):\n",
    "    \"\"\"Friendly confusion matrix; saves to file and (optionally) shows inline.\"\"\"\n",
    "    if normalize:\n",
    "        with np.errstate(all=\"ignore\"):\n",
    "            cmn = cm.astype(\"float\") / cm.sum(axis=1, keepdims=True)\n",
    "            cmn = np.nan_to_num(cmn)\n",
    "    else:\n",
    "        cmn = cm\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.imshow(cmn, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(\"Confusion Matrix\" + (\" (normalized)\" if normalize else \"\"))\n",
    "    plt.colorbar()\n",
    "    ticks = np.arange(len(class_names))\n",
    "    plt.xticks(ticks, class_names, rotation=45, ha=\"right\")\n",
    "    plt.yticks(ticks, class_names)\n",
    "\n",
    "    thresh = cmn.max() / 2.0\n",
    "    for i, j in itertools.product(range(cmn.shape[0]), range(cmn.shape[1])):\n",
    "        val = cmn[i, j] if normalize else cm[i, j]\n",
    "        txt_color = \"white\" if cmn[i, j] > thresh else \"black\"\n",
    "        plt.text(j, i, f\"{val:.2f}\" if normalize else f\"{val}\",\n",
    "                 ha=\"center\", va=\"center\", color=txt_color, fontsize=9)\n",
    "\n",
    "    plt.ylabel(\"True label\"); plt.xlabel(\"Predicted label\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png_path, dpi=160)\n",
    "    if show: plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def save_model_summary(model, out_txt_path):\n",
    "    buff = io.StringIO()\n",
    "    model.summary(print_fn=lambda s: buff.write(s + \"\\n\"))  # capture\n",
    "    summary_txt = buff.getvalue()\n",
    "    with open(out_txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(summary_txt)\n",
    "\n",
    "def run_experiment(cfg, tag=\"baseline\"):\n",
    "    run_id = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + \"_\" + tag\n",
    "    run_dir = TRIALS / run_id\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # freeze params used\n",
    "    with open(run_dir / \"params.json\", \"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "    model = build_model(cfg)\n",
    "\n",
    "    # save model summary\n",
    "    save_model_summary(model, run_dir / \"model_summary.txt\")\n",
    "\n",
    "    cbs = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=8, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.5),\n",
    "        tf.keras.callbacks.ModelCheckpoint(str(run_dir / \"model.keras\"), monitor=\"val_accuracy\", save_best_only=True),\n",
    "    ]\n",
    "    class_weight = compute_class_weights(train_ds)\n",
    "\n",
    "    hist = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=cfg[\"epochs\"],\n",
    "        class_weight=class_weight,\n",
    "        callbacks=cbs,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # show + save accuracy & loss plots\n",
    "    plot_accuracy(hist, run_dir / \"accuracy.png\", show=True)\n",
    "    plot_loss(hist,     run_dir / \"loss.png\",     show=True)\n",
    "\n",
    "    # Save history json\n",
    "    with open(run_dir / \"history.json\", \"w\") as f:\n",
    "        json.dump({k: [float(x) for x in v] for k, v in hist.history.items()}, f, indent=2)\n",
    "\n",
    "    # Evaluate\n",
    "    val_metrics = model.evaluate(val_ds, verbose=0)\n",
    "    test_metrics = model.evaluate(test_ds, verbose=0)\n",
    "\n",
    "    # Detailed predictions for test set\n",
    "    y_true, y_pred = [], []\n",
    "    for xb, yb in tf.data.Dataset.unbatch(test_ds):\n",
    "        probs = model.predict(tf.expand_dims(xb, 0), verbose=0)[0]\n",
    "        y_true.append(int(yb.numpy()))\n",
    "        y_pred.append(int(np.argmax(probs)))\n",
    "\n",
    "    test_f1_macro = float(f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "    # Classification report (save file; also print short summary)\n",
    "    report = classification_report(y_true, y_pred, target_names=CLASS_NAMES)\n",
    "    print(\"\\nClassification report (test):\\n\", report)\n",
    "    with open(run_dir / \"classification_report.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # save + show Confusion matrices: raw & normalized\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion(cm, CLASS_NAMES, run_dir / \"confusion_matrix_raw.png\", normalize=False, cmap=\"Blues\", show=True)\n",
    "    plot_confusion(cm, CLASS_NAMES, run_dir / \"confusion_matrix_norm.png\", normalize=True,  cmap=\"Blues\", show=True)\n",
    "\n",
    "    # Summaries\n",
    "    val_acc  = float(val_metrics[1])\n",
    "    test_acc = float(test_metrics[1])\n",
    "    metrics = {\n",
    "        \"val_accuracy\": val_acc,\n",
    "        \"test_accuracy\": test_acc,\n",
    "        \"test_f1_macro\": test_f1_macro,\n",
    "        \"epochs_trained\": int(len(hist.history[\"loss\"]))\n",
    "    }\n",
    "    with open(run_dir / \"metrics.json\", \"w\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # Leaderboard row\n",
    "    with open(LEADERBOARD, \"a\", newline=\"\") as f:\n",
    "        csv.writer(f).writerow([\n",
    "            run_id, tag,\n",
    "            f\"{val_acc:.4f}\",\n",
    "            f\"{float(f1_score(y_true, y_pred, average='macro')):.4f}\",\n",
    "            f\"{test_acc:.4f}\",\n",
    "            f\"{test_f1_macro:.4f}\",\n",
    "            metrics[\"epochs_trained\"],\n",
    "            json.dumps(cfg)\n",
    "        ])\n",
    "\n",
    "    print(\"Saved trial to:\", run_dir)\n",
    "    return run_id, metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb481a",
   "metadata": {},
   "source": [
    "## Run baseline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c54e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy so later optimizations won't recast BASELINE\n",
    "baseline_cfg = json.loads(json.dumps(BASELINE))\n",
    "run_id, metrics = run_experiment(baseline_cfg, tag=\"baseline\")\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e5d54",
   "metadata": {},
   "source": [
    "## Run optimization: Learning Rate Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e097d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Reduction\n",
    "lr_slow_cfg = json.loads(json.dumps(BASELINE))\n",
    "lr_slow_cfg[\"learning_rate\"] = 2.5e-4\n",
    "run_id, metrics = run_experiment(lr_slow_cfg, tag=\"lr_2p5e-4\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9057e9e0",
   "metadata": {},
   "source": [
    "## Run optimization: Higher Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c69305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher Dropout (stronger regularization)\n",
    "dropout_high_cfg = json.loads(json.dumps(BASELINE))\n",
    "dropout_high_cfg[\"dropout\"] = 0.5\n",
    "run_id, metrics = run_experiment(dropout_high_cfg, tag=\"dropout_0p5\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84b653",
   "metadata": {},
   "source": [
    "## Run optimization: Lower Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d117e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Dropout (weaker regularization)\n",
    "dropout_low_cfg = json.loads(json.dumps(BASELINE))\n",
    "dropout_low_cfg[\"dropout\"] = 0.3\n",
    "run_id, metrics = run_experiment(dropout_low_cfg, tag=\"dropout_0p3\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f9e906",
   "metadata": {},
   "source": [
    "## Run optimization: Reduced Filter Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00538453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced Filter Depth (lighter model)\n",
    "filters_light_cfg = json.loads(json.dumps(BASELINE))\n",
    "filters_light_cfg[\"filters\"] = [32, 64, 96, 128]\n",
    "run_id, metrics = run_experiment(filters_light_cfg, tag=\"filters_light\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f5b74",
   "metadata": {},
   "source": [
    "## Run optimization: Increased Early Filter Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increased Early Filter Depth (richer low-level features)\n",
    "filters_richfront_cfg = json.loads(json.dumps(BASELINE))\n",
    "filters_richfront_cfg[\"filters\"] = [48, 64, 128, 256]\n",
    "run_id, metrics = run_experiment(filters_richfront_cfg, tag=\"filters_richfront\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec81b3e",
   "metadata": {},
   "source": [
    "## Run optimization: Smaller Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e392a599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller Batch Size (adds gradient noise regularization)\n",
    "batch32_cfg = json.loads(json.dumps(BASELINE))\n",
    "batch32_cfg[\"batch_size\"] = 32\n",
    "run_id, metrics = run_experiment(batch32_cfg, tag=\"batch32\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afeacc2d",
   "metadata": {},
   "source": [
    "## Run optimization: Larger Batch Size with Adjusted LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c06dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Larger Batch with Adjusted LR (stability focus)\n",
    "batch128_cfg = json.loads(json.dumps(BASELINE))\n",
    "batch128_cfg[\"batch_size\"] = 128\n",
    "batch128_cfg[\"learning_rate\"] = 5e-4\n",
    "run_id, metrics = run_experiment(batch128_cfg, tag=\"batch128_lr5e-4\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51c7c7",
   "metadata": {},
   "source": [
    "## Run optimization: Stronger L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd6f359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stronger L2 Regularization (weight decay)\n",
    "l2_stronger_cfg = json.loads(json.dumps(BASELINE))\n",
    "l2_stronger_cfg[\"l2\"] = 5e-4\n",
    "run_id, metrics = run_experiment(l2_stronger_cfg, tag=\"l2_5e-4\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e476f4",
   "metadata": {},
   "source": [
    "## Run optimization: Enhanced Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Data Augmentation (data-driven regularization)\n",
    "aug_stronger_cfg = json.loads(json.dumps(BASELINE))\n",
    "aug_stronger_cfg[\"augment\"][\"rotation\"] = 0.15   # baseline was 0.08\n",
    "aug_stronger_cfg[\"augment\"][\"zoom\"] = 0.20       # baseline was 0.10\n",
    "run_id, metrics = run_experiment(aug_stronger_cfg, tag=\"aug_stronger\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4006b6",
   "metadata": {},
   "source": [
    "## Run optimization: Lower Dropout with Stronger L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54672e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Dropout with Stronger L2 Regularization\n",
    "dropout03_l2_cfg = json.loads(json.dumps(BASELINE))\n",
    "dropout03_l2_cfg[\"dropout\"] = 0.3\n",
    "dropout03_l2_cfg[\"l2\"] = 5e-4\n",
    "run_id, metrics = run_experiment(dropout03_l2_cfg, tag=\"dropout_0p3_l2_5e-4\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d0ea0",
   "metadata": {},
   "source": [
    "## Run optimization: Higher Dropout with Stronger L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9ac9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher Dropout with Stronger L2 Regularization\n",
    "dropout05_l2_cfg = json.loads(json.dumps(BASELINE))\n",
    "dropout05_l2_cfg[\"dropout\"] = 0.5\n",
    "dropout05_l2_cfg[\"l2\"] = 5e-4\n",
    "run_id, metrics = run_experiment(dropout05_l2_cfg, tag=\"dropout_0p5_l2_5e-4\")\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a97fc8",
   "metadata": {},
   "source": [
    "## Run optimization: Lower Dropout with Smaller Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9ad3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower Dropout with Smaller Batch Size\n",
    "dropout03_b32_cfg = json.loads(json.dumps(BASELINE))\n",
    "dropout03_b32_cfg[\"dropout\"] = 0.3\n",
    "dropout03_b32_cfg[\"batch_size\"] = 32\n",
    "run_id, metrics = run_experiment(dropout03_b32_cfg, tag=\"dropout_0p3_batch32\")\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04c944d",
   "metadata": {},
   "source": [
    "## Run optimization: Increased Early Filter Depth with Higher Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec81821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increased Early Filter Depth with Higher Dropout\n",
    "richfront_dropout05_cfg = json.loads(json.dumps(BASELINE))\n",
    "richfront_dropout05_cfg[\"filters\"] = [48, 64, 128, 256]\n",
    "richfront_dropout05_cfg[\"dropout\"] = 0.5\n",
    "run_id, metrics = run_experiment(richfront_dropout05_cfg, tag=\"filters_richfront_dropout_0p5\")\n",
    "metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fe2d76",
   "metadata": {},
   "source": [
    "## Promote best-performing model to /artifacts/models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbeb4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Promote best-performing model\n",
    "\n",
    "lb_path = OUTS / \"leaderboard.csv\"\n",
    "lb = pd.read_csv(lb_path)\n",
    "\n",
    "# Choose metric to rank by\n",
    "best_row = lb.sort_values(\"test_f1_macro\", ascending=False).iloc[0]\n",
    "best_run_id = best_row[\"run_id\"]\n",
    "\n",
    "src_model = TRIALS / best_run_id / \"model.keras\"\n",
    "dst_model = MODELS / \"best_model.keras\"\n",
    "\n",
    "shutil.copy2(src_model, dst_model)\n",
    "print(f\"Promoted best model:\\n  From: {src_model}\\n  To:   {dst_model}\")\n",
    "\n",
    "# Save its metrics for quick reference\n",
    "with open(TRIALS / best_run_id / \"metrics.json\") as f:\n",
    "    best_metrics = json.load(f)\n",
    "\n",
    "with open(MODELS / \"best_model_metrics.json\", \"w\") as f:\n",
    "    json.dump(best_metrics, f, indent=2)\n",
    "\n",
    "print(\"Saved metrics snapshot:\", (MODELS / \"best_model_metrics.json\").resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EmotionRecognitionCNN)",
   "language": "python",
   "name": "emotionrecognitioncnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
